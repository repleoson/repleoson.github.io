---
title: "Introduction to Continual Learning - Davide Abati (CVPR 2020)"
image: "https:\/\/i.ytimg.com\/vi\/k0kMx4BFLmI\/hqdefault.jpg"
vid_id: "k0kMx4BFLmI"
categories: "Science-Technology"
tags: ["machine learning","neural networks","deep learning"]
date: "2021-10-04T20:46:53+03:00"
vid_date: "2020-11-15T18:34:21Z"
duration: "PT1H31M27S"
viewcount: "2016"
likeCount: "54"
dislikeCount: "0"
channel: "2d3d.ai"
---
{% raw %}This talk introduce Continual Learning in general and a deep dive into the CVPR 2020 paper &quot;Conditional Channel Gated Networks for Task-Aware Continual Learning&quot;.<br /><br />Wiki: <a rel="nofollow" target="blank" href="https://wiki.continualai.org">https://wiki.continualai.org</a><br />Arxiv: <a rel="nofollow" target="blank" href="https://arxiv.org/abs/2004.00070">https://arxiv.org/abs/2004.00070</a><br /><br />References to everything covered in the lecture: <a rel="nofollow" target="blank" href="https://www.reddit.com/r/2D3DAI/comments/js69za/references_from_lecture_introduction_to_continual/">https://www.reddit.com/r/2D3DAI/comments/js69za/references_from_lecture_introduction_to_continual/</a><br /><br />00:00 Intro<br />04:32 What is Continual Learning (CL)?<br />06:16 Catastrophic forgetting08:07 Tasks • Usually, each task is in the form of a classification dataset<br />12:59 Continual learning: timeline<br />13:58 Continual Learning desiderata<br />16:12 Related works<br />17:37 Progressive Neural Networks<br />23:29 Learning without Forgetting<br />27:58 Elastic Weight Consolidation<br />29:00 Regularization approaches<br />30:01 Gradient Episodic Memory (GEM)<br />39:52 PackNet<br />40:05 Parameter isolation methods<br />40:55 Conditional Channel Gated Networks for Task-Aware Continual Learning CVPR 2020<br />42:26 Motivation<br />43:16 Joint prediction of task and class<br />45:05 Task-incremental learning of class labels<br />48:51 Sparsity dynamics<br />01:02:00 Class incremental learning of task labels<br />01:09:10 Episodic or generative memory?<br />01:14:20 Limitations<br />01:20:14 Conclusions and Future directions for continual learning<br />01:23:00 Discussion<br /><br />[Chapters were auto-generated using our proprietary software - contact us if you are interested in access to the software]<br /><br />Lecture abstract:<br /><br />Neural networks struggle to learn continuously and experience catastrophic forgetting when optimized on a sequence of learning problems. As such, whenever the training distribution shifts, they overwrite the old knowledge to fit the current examples. Continual Learning (CL) is the research area addressing the forgetting problem in learning models, and it has inspired a plethora of approaches and evaluation settings. This talk will discuss several successful strategies as well as some of their drawbacks. Moreover, we will introduce a CL model based on conditional computation: by equipping each layer with task-specific gating modules, the network can autonomously select which units to apply at the given input. By monitoring the activation patterns of such gates, we can identify important units for the current task and freeze them before proceeding to the next one, ensuring no loss in performance. An extension will also be discussed, capable of dealing with the more general case in which, at test time, examples do not come with associated task labels.<br /><br />Presenter BIO:<br /><br />Davide Abati is a machine learning researcher at Qualcomm AI Research, based in Amsterdam. He holds a master's degree in computer engineering from the University of Modena and Reggio Emilia, where he also pursued his Ph.D. in computer vision advised by Prof. Rita Cucchiara. His research focuses on different areas of computer vision, spanning from human attention prediction to novelty detection and continual learning. Some of his works were published in top-tier conferences and journals, such as CVPR, Neurips, and TPAMI. He also regularly serves as a reviewer for several IEEE transactions journals.<br /><br />His website: <a rel="nofollow" target="blank" href="https://davideabati.info">https://davideabati.info</a><br /><br />-------------------------<br />Find us at:<br /><br />Newsletter for updates about more events ➜ <a rel="nofollow" target="blank" href="http://eepurl.com/gJ1t-D">http://eepurl.com/gJ1t-D</a><br />Sub-reddit for discussions ➜ <a rel="nofollow" target="blank" href="https://www.reddit.com/r/2D3DAI/">https://www.reddit.com/r/2D3DAI/</a><br />Discord server for, well, discord ➜ <a rel="nofollow" target="blank" href="https://discord.gg/MZuWSjF">https://discord.gg/MZuWSjF</a><br />Blog ➜ <a rel="nofollow" target="blank" href="https://2d3d.ai">https://2d3d.ai</a><br /><br />We are the people behind the AI consultancy Abelians ➜ <a rel="nofollow" target="blank" href="https://abelians.com/">https://abelians.com/</a>{% endraw %}
